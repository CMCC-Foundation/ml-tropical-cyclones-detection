{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine_vector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../resources/library')\n",
    "from tropical_cyclone.georeferencing import round_to_grid\n",
    "from tropical_cyclone.cyclone import init_track_dataframe, tracking_algorithm, paper_tracking_algorithm, track_matching\n",
    "from tropical_cyclone.visualize import plot_tracks, plot_detections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inference directory to draw detections\n",
    "dataset_dir = '../data/inference'\n",
    "available_models = sorted([folder for folder in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, folder))])\n",
    "available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model to analyze\n",
    "selected_model = '11_swin_fg10_t_500_msl_vo_850'\n",
    "\n",
    "# get ibtracs directory\n",
    "ibtracs_src = '../data/ibtracs/filtered/ibtracs_main-tracks_6h_1980-2021_TS-NR-ET-MX-SS-DS.csv'\n",
    "\n",
    "# define test years (same as paper)\n",
    "test_years = [i for i in range(1980,2020)]\n",
    "#Â test_years = [1983, 1984, 1993, 1994, 2003, 2004, 2013, 2014]\n",
    "# test_years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "# test_years = [1993]\n",
    "\n",
    "# kilometer threshold\n",
    "max_distance_detection = 1000.0\n",
    "\n",
    "# whether or not to make plots\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model directory\n",
    "model_dir = os.path.join(dataset_dir, selected_model)\n",
    "# get inference filenames\n",
    "inference_files = [os.path.join(model_dir, f'{year}.csv') for year in test_years]\n",
    "model_dir, inference_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv files\n",
    "csv_files = []\n",
    "for file in inference_files:\n",
    "    csv_files.append(pd.read_csv(file, index_col=0))\n",
    "# merge csv files together\n",
    "detections = pd.concat(csv_files).reset_index(drop=True)\n",
    "# convert iso time with pandas\n",
    "detections['ISO_TIME'] = pd.to_datetime(detections['ISO_TIME'])\n",
    "# add WS as np.inf\n",
    "detections['WS'] = np.inf\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ISO_TIME','SID','NATURE','WMO_WIND','LAT','LON']\n",
    "# load ibtracs\n",
    "observations = pd.read_csv(ibtracs_src, index_col=0)\n",
    "# convert iso time with pandas\n",
    "observations['ISO_TIME'] = pd.to_datetime(observations['ISO_TIME'])\n",
    "# get only some columns from ibtracs\n",
    "observations = observations[columns]\n",
    "# round lat and lon to be comparable with training data\n",
    "observations['LAT'] = round_to_grid(observations['LAT'], grid_res=0.25)\n",
    "observations['LON'] = round_to_grid(observations['LON'], grid_res=0.25)\n",
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.merge(left=detections, right=observations, on='ISO_TIME', how='inner')\n",
    "tmp = tmp[tmp['ISO_TIME'].dt.year.isin(test_years)]\n",
    "dates = tmp['ISO_TIME'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(observations)} observations and {len(detections)} detections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only detections and observations present on both dataframes\n",
    "detections = detections[detections['ISO_TIME'].isin(dates)].reset_index(drop=True)\n",
    "observations = observations[observations['ISO_TIME'].isin(dates)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(observations)} observations and {len(detections)} detections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge together detections and ibtracs\n",
    "matches = pd.merge(left=detections, right=observations, on='ISO_TIME')\n",
    "# compute haversine distance between any couple of points\n",
    "matches['HDIST'] = haversine_vector(array1=matches[['LAT_x','LON_x']].to_numpy(), array2=matches[['LAT_y','LON_y']].to_numpy(), normalize=True)\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the distances above `max_distance_localization` km\n",
    "matches = matches[matches['HDIST'] < max_distance_detection]\n",
    "# group by LATx and LONx and find the minimum (to remove x duplicates)\n",
    "matches = matches.groupby(by=['ISO_TIME','LAT_x','LON_x','SID','NATURE','WMO_WIND']).min('HDIST').reset_index()\n",
    "# repeat grouping by LATy and LONy and find the minimum (to remove y duplicates)\n",
    "matches = matches.groupby(by=['ISO_TIME','LAT_y','LON_y','SID','NATURE','WMO_WIND']).min('HDIST').reset_index()\n",
    "# show result\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance_localization = matches['HDIST'].min()\n",
    "max_distance_localization = matches['HDIST'].max()\n",
    "mean_distance_localization = matches['HDIST'].mean()\n",
    "median_distance_localization = matches['HDIST'].median()\n",
    "\n",
    "print(f\"Model {selected_model} Localization results\")\n",
    "print(f\"   Min distance ({np.round(min_distance_localization,2)} km)\")\n",
    "print(f\"   Max distance ({np.round(max_distance_localization,2)} km)\")\n",
    "print(f\"   Average distance ({np.round(mean_distance_localization,2)} km)\")\n",
    "print(f\"   Median distance ({np.round(median_distance_localization,2)} km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot: plot_detections(detections, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_beta(beta, precision, recall):\n",
    "    return (1 + beta**2) * ((precision * recall) / ((beta**2 * precision) + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dets = len(detections)\n",
    "n_tp = len(matches)\n",
    "n_obs = len(observations)\n",
    "n_fp = n_dets - n_tp\n",
    "n_fn = n_obs - n_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = n_tp / (n_tp + n_fp)\n",
    "recall = n_tp / (n_tp + n_fn)\n",
    "f2_score = F_beta(beta=2, precision=precision, recall=recall) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model {selected_model} Classification results\")\n",
    "print(f\"   F2 : {np.round(f2_score,2)} % (precision={np.round(precision, 2)}, recall={np.round(recall,2)})\")\n",
    "print(f\"   TP : {n_tp} out of {n_obs} observations ({np.round(n_tp / n_obs * 100)} %)\")\n",
    "print(f\"   FP : {n_fp} out of {n_dets} ML detections ({np.round(n_fp / n_dets * 100)} %)\")\n",
    "print(f\"   FN : {n_fn} out of {n_obs} observations ({np.round(n_fn / n_obs * 100)} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum track length (1 day)\n",
    "min_track_count = 12\n",
    "# minimum speed of wind in order to consider the track true\n",
    "min_wind_speed = 17.0\n",
    "# maximum distance between tracks\n",
    "max_track_distance_tracking = 400.0\n",
    "\n",
    "grid_res = 0.25\n",
    "km_to_deg = 110.474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename SID to TRACK_ID\n",
    "observed_tracks = observations.rename(columns={'SID':'TRACK_ID'})\n",
    "# get only long enough tracks for the comparison\n",
    "valid_observations_sids = observed_tracks.groupby('TRACK_ID').filter(lambda x: len(x) >= min_track_count)['TRACK_ID'].unique()\n",
    "# filter out the observations\n",
    "observed_tracks = observed_tracks[observed_tracks['TRACK_ID'].isin(valid_observations_sids)].reset_index(drop=True)\n",
    "observed_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tracking scheme\n",
    "tracking_src = f'/Users/davide/Developer/ml-tropical-cyclones-detection/data/inference/{selected_model}/tracking.csv'\n",
    "if not os.path.exists(tracking_src):\n",
    "    detected_tracks = init_track_dataframe(detections)\n",
    "    detected_tracks = tracking_algorithm(detected_tracks, max_track_distance_tracking, min_track_count)\n",
    "    # detected_tracks = paper_tracking_algorithm(detected_tracks, max_track_distance_tracking, min_track_count)\n",
    "    detected_tracks.to_csv(tracking_src)\n",
    "else:\n",
    "    detected_tracks = pd.read_csv(tracking_src, index_col=0)\n",
    "detected_tracks.head()\n",
    "\n",
    "# detected_tracks = init_track_dataframe(detections)\n",
    "# detected_tracks = tracking_algorithm(detected_tracks, max_track_distance_tracking, min_track_count)\n",
    "# detected_tracks = paper_tracking_algorithm(detected_tracks, max_track_distance_tracking, min_track_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are:')\n",
    "print(f'   - {len(detected_tracks[\"TRACK_ID\"].unique())} detected tracks')\n",
    "# print(f'   - {len(paper_detected_tracks[\"TRACK_ID\"].unique())} detected tracks (paper)')\n",
    "print(f'   - {len(observed_tracks[\"TRACK_ID\"].unique())} observed tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:plot_tracks(detected_tracks, observed_tracks)\n",
    "# plot_tracks(detected_tracks[pd.to_datetime(detected_tracks['ISO_TIME']).dt.year.isin([1993])], observed_tracks[pd.to_datetime(observed_tracks['ISO_TIME']).dt.year.isin([1993])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/davide/Developer/ml-tropical-cyclones-detection/resources/library/dynamicopy-0.6.1')\n",
    "import dynamicopy\n",
    "\n",
    "# maximum distance to consider true the match\n",
    "max_track_distance_matching = 300.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our track matching algorithm provides same results of Bourdin's (but more slowly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track_matches = track_matching(detected_tracks, observed_tracks, max_track_distance_matching)\n",
    "# # H = HITS = True Positive\n",
    "# H = len(track_matches[(track_matches['DET_TRACK_ID']!='') & (track_matches['OBS_TRACK_ID']!='')])\n",
    "# # M = Miss = False Negative\n",
    "# M = len(track_matches[(track_matches['DET_TRACK_ID']=='') & (track_matches['OBS_TRACK_ID']!='')])\n",
    "# # FA = False Alarm = False Positive\n",
    "# FA = len(track_matches[(track_matches['DET_TRACK_ID']!='') & (track_matches['OBS_TRACK_ID']=='')])\n",
    "\n",
    "# POD = (H / (H + M))\n",
    "# FAR = (FA / (H + FA))\n",
    "\n",
    "# print(f\"Hits : {H}\")\n",
    "# print(f\"Miss : {M}\")\n",
    "# print(f\"False Alarm : {FA}\")\n",
    "# print(f\"POD : {POD}\")\n",
    "# print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load bourdin observed tracks from library\n",
    "# bourdin_observed_tracks = pd.read_csv('/Users/davide/Developer/ml-tropical-cyclones-detection/resources/library/zenodo_bourdin/ibtracs/ibtracs.since1980.cleaned.csv', index_col=0)\n",
    "\n",
    "# # convert columns to dynamicopy compliant format\n",
    "# detected_tracks = detected_tracks.rename(columns={'ISO_TIME':'time','LAT':'lat','LON':'lon','TRACK_ID':'track_id'})\n",
    "# observed_tracks = observed_tracks.rename(columns={'ISO_TIME':'time','LAT':'lat','LON':'lon','TRACK_ID':'track_id'})\n",
    "\n",
    "# # convert time column pandas datetime format\n",
    "# detected_tracks['time'] = pd.to_datetime(detected_tracks['time'])\n",
    "# observed_tracks['time'] = pd.to_datetime(observed_tracks['time'])\n",
    "# bourdin_observed_tracks['time'] = pd.to_datetime(bourdin_observed_tracks['time'])\n",
    "\n",
    "# # convert longitudes to range [0, 360] format\n",
    "# detected_tracks['lon'] = (detected_tracks['lon'] + 540) % 360 - 180\n",
    "# observed_tracks['lon'] = (observed_tracks['lon'] + 540) % 360 - 180\n",
    "# bourdin_observed_tracks['lon'] = (bourdin_observed_tracks['lon'] + 540) % 360 - 180\n",
    "\n",
    "# # remove out of bound (both space and time) detections from bourdin observations\n",
    "# bourdin_observed_tracks = bourdin_observed_tracks[(bourdin_observed_tracks['lon']>=100) & (bourdin_observed_tracks['lon']<=320) & (bourdin_observed_tracks['lat']>=0) & (bourdin_observed_tracks['lat']<=70)]\n",
    "# bourdin_observed_tracks = bourdin_observed_tracks[bourdin_observed_tracks['time'].isin(dates)]\n",
    "# bourdin_observed_tracks = bourdin_observed_tracks[bourdin_observed_tracks['track_id'].isin(bourdin_observed_tracks.groupby('track_id').filter(lambda x: len(x) >= min_track_count)['track_id'].unique())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bourdin_track_matches = dynamicopy.match_tracks(detected_tracks, bourdin_observed_tracks, \"ours\", 'bourdin', max_dist=max_track_distance_matching, min_overlap=0, ref=True)\n",
    "\n",
    "# n_ib_match = len(bourdin_track_matches[f'id_bourdin'].unique())\n",
    "# n_our_match = len(bourdin_track_matches['id_ours'].unique())\n",
    "# n_observations = len(bourdin_observed_tracks.track_id.unique())\n",
    "# n_detections = len(detected_tracks.track_id.unique())\n",
    "\n",
    "# H, M, FA = n_ib_match, n_observations - n_ib_match, n_detections - n_our_match\n",
    "# POD = H / (H + M)\n",
    "# FAR = FA / (H + FA)\n",
    "\n",
    "# print(f\"Hits : {H}\")\n",
    "# print(f\"Misses : {M}\")\n",
    "# print(f\"False Alarms : {FA}\")\n",
    "# print(f\"POD : {POD}\")\n",
    "# print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matches = dynamicopy.match_tracks(detected_tracks, observed_tracks, \"ours\", 'ibtracs', max_dist=max_track_distance_matching, min_overlap=0, ref=True)\n",
    "\n",
    "n_ib_match = len(track_matches[f'id_ibtracs'].unique())\n",
    "n_our_match = len(track_matches['id_ours'].unique())\n",
    "n_observations = len(observed_tracks.track_id.unique())\n",
    "n_detections = len(detected_tracks.track_id.unique())\n",
    "\n",
    "H, M, FA = n_ib_match, n_observations - n_ib_match, n_detections - n_our_match\n",
    "POD = H / (H + M)\n",
    "FAR = FA / (H + FA)\n",
    "\n",
    "# POD, FAR, H, M, FA\n",
    "print(f\"Hits : {H}\")\n",
    "print(f\"Misses : {M}\")\n",
    "print(f\"False Alarms : {FA}\")\n",
    "print(f\"POD : {POD}\")\n",
    "print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'model', 'max_distance_detection', 'n_dets', 'n_tp', 'n_obs', 'n_fp', 'n_fn', 'precision', \n",
    "    'recall', 'f2_score', 'min_distance_localization', 'max_distance_localization', \n",
    "    'mean_distance_localization', 'median_distance_localization', 'min_track_count', \n",
    "    'max_distance_tracking', 'min_wind_speed', 'max_track_distance_matching', \n",
    "    'max_track_distance_tracking', 'H', 'M', 'FA', 'POD', 'FAR', 'ibtracs_src', 'test_years', \n",
    "]\n",
    "dst = '/Users/davide/Developer/ml-tropical-cyclones-detection/data/inference/results_analysis.csv'\n",
    "\n",
    "if os.path.exists(dst):\n",
    "    results = pd.read_csv(dst, index_col=0)\n",
    "else:\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "\n",
    "results = pd.concat([results, pd.DataFrame(data={\n",
    "    'model': [selected_model], 'max_distance_detection': [max_distance_detection], 'n_dets': [n_dets], 'n_tp': [n_tp], \n",
    "    'n_obs': [n_obs], 'n_fp': [n_fp], 'n_fn': [n_fn], 'precision': [precision], 'recall': [recall], \n",
    "    'f2_score': [f2_score], 'min_distance_localization': [min_distance_localization], 'max_distance_localization': [max_distance_localization], \n",
    "    'mean_distance_localization': [mean_distance_localization], 'median_distance_localization': [median_distance_localization], \n",
    "    'min_track_count': [min_track_count], 'max_distance_tracking': [max_track_distance_tracking], 'min_wind_speed': [min_wind_speed], \n",
    "    'max_track_distance_matching': [max_track_distance_matching], 'max_track_distance_tracking': [max_track_distance_tracking], \n",
    "    'H': [H], 'M': [M], 'FA': [FA], 'POD': [POD], 'FAR': [FAR], 'ibtracs_src': [ibtracs_src], 'test_years': [test_years], \n",
    "})])\n",
    "\n",
    "results = results.reset_index(drop=True)\n",
    "#Â results.to_csv(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Results\n",
    "\n",
    "In the paper, with the ML ensemble we have the following results:\n",
    "\n",
    "- F2-score : 53 %\n",
    "- Euclidean distance : 117.06 km\n",
    "- Hit rate : 88.91 %\n",
    "- POD : 71.49 %\n",
    "- FAR : 23.00 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
