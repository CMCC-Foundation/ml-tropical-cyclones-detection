{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine_vector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../resources/library')\n",
    "from tropical_cyclone.georeferencing import round_to_grid\n",
    "from tropical_cyclone.cyclone import init_track_dataframe, tracking_algorithm, track_matching\n",
    "from tropical_cyclone.visualize import plot_tracks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model to analyze\n",
    "# selected_model = '02_swin_msl_vo_850'\n",
    "# selected_model = '03_vgg_v3_relu_ks3_msl_vo_850'\n",
    "# selected_model = '04_vgg_v3_relu_ks5_msl_vo_850'\n",
    "# selected_model = '05_vgg_v3_linear_ks3_msl_vo_850'\n",
    "# selected_model = '06_swin_msl_vo_850'\n",
    "selected_model = '07_vgg_v3_silu_ks3_msl_vo_850'\n",
    "\n",
    "# define inference directory to draw detections\n",
    "dataset_dir = '../data/inference'\n",
    "# get ibtracs directory\n",
    "ibtracs_src = '../data/ibtracs/filtered/ibtracs_main-tracks_6h_1980-2021_TS-NR-ET-MX-SS-DS.csv'\n",
    "# define test years (same as paper)\n",
    "test_years = [i for i in range(1980,2020)]\n",
    "# test_years = [1983, 1984, 1993, 1994, 2003, 2004, 2013, 2014]\n",
    "# test_years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "# test_years = [1993]\n",
    "# kilometer threshold\n",
    "max_distance_detection = 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model directory\n",
    "model_dir = os.path.join(dataset_dir, selected_model)\n",
    "# get inference filenames\n",
    "inference_files = [os.path.join(model_dir, f'{year}.csv') for year in test_years]\n",
    "model_dir, inference_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv files\n",
    "csv_files = []\n",
    "for file in inference_files:\n",
    "    csv_files.append(pd.read_csv(file, index_col=0))\n",
    "# merge csv files together\n",
    "detections = pd.concat(csv_files).reset_index(drop=True)\n",
    "# convert iso time with pandas\n",
    "detections['ISO_TIME'] = pd.to_datetime(detections['ISO_TIME'])\n",
    "# add WS as np.inf\n",
    "detections['WS'] = np.inf\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ISO_TIME','SID','NATURE','WMO_WIND','LAT','LON']\n",
    "# load ibtracs\n",
    "observations = pd.read_csv(ibtracs_src, index_col=0)\n",
    "# convert iso time with pandas\n",
    "observations['ISO_TIME'] = pd.to_datetime(observations['ISO_TIME'])\n",
    "# get only some columns from ibtracs\n",
    "observations = observations[columns]\n",
    "# round lat and lon to be comparable with training data\n",
    "observations['LAT'] = round_to_grid(observations['LAT'], grid_res=0.25)\n",
    "observations['LON'] = round_to_grid(observations['LON'], grid_res=0.25)\n",
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.merge(left=detections, right=observations, on='ISO_TIME', how='inner')\n",
    "tmp = tmp[tmp['ISO_TIME'].dt.year.isin(test_years)]\n",
    "dates = tmp['ISO_TIME'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(observations)} observations and {len(detections)} detections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only detections and observations present on both dataframes\n",
    "detections = detections[detections['ISO_TIME'].isin(dates)].reset_index(drop=True)\n",
    "observations = observations[observations['ISO_TIME'].isin(dates)].reset_index(drop=True)\n",
    "\n",
    "# select only TCs belonging to a certain nature\n",
    "# observations = observations[observations['NATURE'].isin(['TS','SS','ET'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(observations)} observations and {len(detections)} detections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge together detections and ibtracs\n",
    "matches = pd.merge(left=detections, right=observations, on='ISO_TIME')\n",
    "# compute haversine distance between any couple of points\n",
    "matches['HDIST'] = haversine_vector(array1=matches[['LAT_x','LON_x']].to_numpy(), array2=matches[['LAT_y','LON_y']].to_numpy(), normalize=True)\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the distances above `max_distance_localization` km\n",
    "matches = matches[matches['HDIST'] < max_distance_detection]\n",
    "# group by LATx and LONx and find the minimum (to remove x duplicates)\n",
    "matches = matches.groupby(by=['ISO_TIME','LAT_x','LON_x','SID','NATURE','WMO_WIND']).min('HDIST').reset_index()\n",
    "# repeat grouping by LATy and LONy and find the minimum (to remove y duplicates)\n",
    "matches = matches.groupby(by=['ISO_TIME','LAT_y','LON_y','SID','NATURE','WMO_WIND']).min('HDIST').reset_index()\n",
    "# show result\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance_localization = matches['HDIST'].min()\n",
    "max_distance_localization = matches['HDIST'].max()\n",
    "mean_distance_localization = matches['HDIST'].mean()\n",
    "median_distance_localization = matches['HDIST'].median()\n",
    "\n",
    "print(f\"Model {selected_model} Localization results\")\n",
    "print(f\"   Min distance ({np.round(min_distance_localization,2)} km)\")\n",
    "print(f\"   Max distance ({np.round(max_distance_localization,2)} km)\")\n",
    "print(f\"   Average distance ({np.round(mean_distance_localization,2)} km)\")\n",
    "print(f\"   Median distance ({np.round(median_distance_localization,2)} km)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_detections(detections, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_beta(beta, precision, recall):\n",
    "    return (1 + beta**2) * ((precision * recall) / ((beta**2 * precision) + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dets = len(detections)\n",
    "n_tp = len(matches)\n",
    "n_obs = len(observations)\n",
    "n_fp = n_dets - n_tp\n",
    "n_fn = n_obs - n_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = n_tp / (n_tp + n_fp)\n",
    "recall = n_tp / (n_tp + n_fn)\n",
    "f2_score = F_beta(beta=2, precision=precision, recall=recall) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model {selected_model} Classification results\")\n",
    "print(f\"   F2 : {np.round(f2_score,2)} % (precision={np.round(precision, 2)}, recall={np.round(recall,2)})\")\n",
    "print(f\"   TP : {n_tp} out of {n_obs} observations ({np.round(n_tp / n_obs * 100)} %)\")\n",
    "print(f\"   FP : {n_fp} out of {n_dets} ML detections ({np.round(n_fp / n_dets * 100)} %)\")\n",
    "print(f\"   FN : {n_fn} out of {n_obs} observations ({np.round(n_fn / n_obs * 100)} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum track length (1 day)\n",
    "min_track_count = 12\n",
    "# maximum distance (in km) between each consecutive tc\n",
    "max_distance_tracking = 400.0\n",
    "# minimum speed of wind in order to consider the track true\n",
    "min_wind_speed = 17.0\n",
    "# maximum distance between matches between tracks\n",
    "max_track_distance_tracking = 300.0\n",
    "\n",
    "grid_res = 0.25\n",
    "km_to_deg = 110.474\n",
    "\n",
    "# whether or not to plot the tracks\n",
    "plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename SID to TRACK_ID\n",
    "observed_tracks = observations.rename(columns={'SID':'TRACK_ID'})\n",
    "# get only long enough tracks for the comparison\n",
    "valid_observations_sids = observed_tracks.groupby('TRACK_ID').filter(lambda x: len(x) >= min_track_count)['TRACK_ID'].unique()\n",
    "# filter out the observations\n",
    "observed_tracks = observed_tracks[observed_tracks['TRACK_ID'].isin(valid_observations_sids)].reset_index(drop=True)\n",
    "observed_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tracking scheme\n",
    "tracking_src = f'/Users/davide/Developer/ml-tropical-cyclones-detection/data/inference/{selected_model}/tracking.csv'\n",
    "if not os.path.exists(tracking_src):\n",
    "    detected_tracks = init_track_dataframe(detections)\n",
    "    detected_tracks = tracking_algorithm(detected_tracks, max_distance_tracking, min_track_count)\n",
    "    detected_tracks.to_csv(tracking_src)\n",
    "else:\n",
    "    detected_tracks = pd.read_csv(tracking_src, index_col=0)\n",
    "# store detected tracks to disk\n",
    "detected_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_detected_tracks = init_track_dataframe(detections)\n",
    "# paper_detected_tracks = paper_tracking_algorithm(paper_detected_tracks, max_distance, min_track_count)\n",
    "# paper_detected_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are:')\n",
    "print(f'   - {len(detected_tracks[\"TRACK_ID\"].unique())} detected tracks')\n",
    "# print(f'   - {len(paper_detected_tracks[\"TRACK_ID\"].unique())} detected tracks (paper)')\n",
    "print(f'   - {len(observed_tracks[\"TRACK_ID\"].unique())} observed tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:plot_tracks(detected_tracks, observed_tracks)\n",
    "# plot_tracks(detected_tracks[pd.to_datetime(detected_tracks['ISO_TIME']).dt.year.isin([2005])], observed_tracks[pd.to_datetime(observed_tracks['ISO_TIME']).dt.year.isin([2005])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_track_distance_matching = 300.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_matches = track_matching(detected_tracks, observed_tracks, max_track_distance_matching)\n",
    "track_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = HITS = True Positive\n",
    "H = len(track_matches[(track_matches['DET_TRACK_ID']!='') & (track_matches['OBS_TRACK_ID']!='')])\n",
    "# M = Miss = False Negative\n",
    "M = len(track_matches[(track_matches['DET_TRACK_ID']=='') & (track_matches['OBS_TRACK_ID']!='')])\n",
    "# FA = False Alarm = False Positive\n",
    "FA = len(track_matches[(track_matches['DET_TRACK_ID']!='') & (track_matches['OBS_TRACK_ID']=='')])\n",
    "\n",
    "POD = (H / (H + M))\n",
    "FAR = (FA / (H + FA))\n",
    "\n",
    "print(f\"Hits : {H}\")\n",
    "print(f\"Miss : {M}\")\n",
    "print(f\"False Alarm : {FA}\")\n",
    "print(f\"POD : {POD}\")\n",
    "print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'model',\n",
    "    'max_distance_detection', \n",
    "    'n_dets', \n",
    "    'n_tp', \n",
    "    'n_obs', \n",
    "    'n_fp', \n",
    "    'n_fn', \n",
    "    'precision', \n",
    "    'recall', \n",
    "    'f2_score', \n",
    "    'min_distance_localization', \n",
    "    'max_distance_localization', \n",
    "    'mean_distance_localization', \n",
    "    'median_distance_localization', \n",
    "    'min_track_count', \n",
    "    'max_distance_tracking', \n",
    "    'min_wind_speed', \n",
    "    'max_track_distance_matching', \n",
    "    'max_track_distance_tracking', \n",
    "    'H', \n",
    "    'M', \n",
    "    'FA', \n",
    "    'POD', \n",
    "    'FAR', \n",
    "    'ibtracs_src', \n",
    "    'test_years', \n",
    "]\n",
    "dst = '/Users/davide/Developer/ml-tropical-cyclones-detection/data/inference/results_analysis.csv'\n",
    "if os.path.exists(dst):\n",
    "    results = pd.read_csv(dst, index_col=0)\n",
    "else:\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, pd.DataFrame(data={\n",
    "    'model': [selected_model],\n",
    "    'max_distance_detection': [max_distance_detection], \n",
    "    'n_dets': [n_dets], \n",
    "    'n_tp': [n_tp], \n",
    "    'n_obs': [n_obs], \n",
    "    'n_fp': [n_fp], \n",
    "    'n_fn': [n_fn], \n",
    "    'precision': [precision], \n",
    "    'recall': [recall], \n",
    "    'f2_score': [f2_score], \n",
    "    'min_distance_localization': [min_distance_localization], \n",
    "    'max_distance_localization': [max_distance_localization], \n",
    "    'mean_distance_localization': [mean_distance_localization], \n",
    "    'median_distance_localization': [median_distance_localization], \n",
    "    'min_track_count': [min_track_count], \n",
    "    'max_distance_tracking': [max_distance_tracking], \n",
    "    'min_wind_speed': [min_wind_speed], \n",
    "    'max_track_distance_matching': [max_track_distance_matching], \n",
    "    'max_track_distance_tracking': [max_track_distance_tracking], \n",
    "    'H': [H], \n",
    "    'M': [M], \n",
    "    'FA': [FA], \n",
    "    'POD': [POD], \n",
    "    'FAR': [FAR], \n",
    "    'ibtracs_src': [ibtracs_src], \n",
    "    'test_years': [test_years], \n",
    "})])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index(drop=True)\n",
    "results.to_csv(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/davide/Developer/ml-tropical-cyclones-detection/resources/library/dynamicopy-0.6.1')\n",
    "import dynamicopy\n",
    "\n",
    "columnsd = {'ISO_TIME': 'time','LAT':'lat','LON':'lon','TRACK_ID':'track_id','WS':'ws'}\n",
    "columns = list(columnsd.values())\n",
    "\n",
    "bobs = dynamicopy.load_ibtracs()\n",
    "bobs = bobs[bobs['basin'].isin(['WNP','ENP','NATL'])].reset_index(drop=True)\n",
    "\n",
    "dets = detected_tracks.rename(columns={'ISO_TIME':'time','LAT':'lat','LON':'lon','TRACK_ID':'track_id'})\n",
    "obss = observed_tracks.rename(columns={'ISO_TIME':'time','LAT':'lat','LON':'lon','TRACK_ID':'track_id'})\n",
    "\n",
    "dets['lon'] = (dets['lon'] + 540) % 360 - 180\n",
    "obss['lon'] = (obss['lon'] + 540) % 360 - 180\n",
    "bobs['lon'] = (bobs['lon'] + 540) % 360 - 180\n",
    "bobs = bobs[(bobs['lon']>=100) & (bobs['lon']<=320) & (bobs['lat']>=0) & (bobs['lat']<=70)]\n",
    "bobs = bobs[bobs['time'].isin(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_bourdin = dynamicopy.match_tracks(dets, bobs, \"ours\", 'bourdin', max_dist=max_track_distance_matching, min_overlap=0, ref=True)\n",
    "\n",
    "n_match = len(match_bourdin[f'id_bourdin'].unique())\n",
    "n_observations = len(bobs.track_id.unique())\n",
    "n_detections = len(dets.track_id.unique())\n",
    "\n",
    "POD = n_match / n_observations\n",
    "FAR = 1 - (n_match / n_detections)\n",
    "H, M, FA = n_match, (n_observations-n_match), n_detections - n_match\n",
    "\n",
    "print(f\"Hits : {H}\")\n",
    "print(f\"Misses : {M}\")\n",
    "print(f\"False Alarms : {FA}\")\n",
    "print(f\"POD : {POD}\")\n",
    "print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_our_ibtracs = dynamicopy.match_tracks(dets, obss, \"ours\", 'ibtracs', max_dist=max_track_distance_matching, min_overlap=0, ref=True)\n",
    "\n",
    "n_match = len(match_our_ibtracs[f'id_ibtracs'].unique())\n",
    "n_observations = len(obss.track_id.unique())\n",
    "n_detections = len(dets.track_id.unique())\n",
    "\n",
    "POD = n_match / n_observations\n",
    "FAR = 1 - (n_match / n_detections)\n",
    "H, M, FA = n_match, (n_observations-n_match), n_detections - n_match\n",
    "\n",
    "# POD, FAR, H, M, FA\n",
    "print(f\"Hits : {H}\")\n",
    "print(f\"Misses : {M}\")\n",
    "print(f\"False Alarms : {FA}\")\n",
    "print(f\"POD : {POD}\")\n",
    "print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_obs_ibtracs = dynamicopy.match_tracks(bobs, obss, \"bourdin\", 'ibtracs', max_dist=max_track_distance_matching, min_overlap=0, ref=True)\n",
    "\n",
    "n_match = min(len(match_obs_ibtracs[f'id_bourdin'].unique()), len(match_obs_ibtracs[f'id_ibtracs'].unique()))\n",
    "n_observations = len(obss.track_id.unique())\n",
    "n_detections = len(bobs.track_id.unique())\n",
    "\n",
    "POD = n_match / n_observations\n",
    "FAR = 1 - (n_match / n_detections)\n",
    "H, M, FA = n_match, (n_observations-n_match), n_detections - n_match\n",
    "\n",
    "# POD, FAR, H, M, FA\n",
    "print(f\"Hits : {H}\")\n",
    "print(f\"Misses : {M}\")\n",
    "print(f\"False Alarms : {FA}\")\n",
    "print(f\"POD : {POD}\")\n",
    "print(f\"FAR : {FAR}\")\n",
    "\n",
    "match_obs_ibtracs = dynamicopy.match_tracks(obss, bobs, \"ibtracs\", 'bourdin', max_dist=max_track_distance_matching, min_overlap=0, ref=True)\n",
    "\n",
    "n_match = min(len(match_obs_ibtracs[f'id_bourdin'].unique()), len(match_obs_ibtracs[f'id_ibtracs'].unique()))\n",
    "n_observations = len(bobs.track_id.unique())\n",
    "n_detections = len(obss.track_id.unique())\n",
    "\n",
    "POD = n_match / n_observations\n",
    "FAR = 1 - (n_match / n_detections)\n",
    "H, M, FA = n_match, (n_observations-n_match), n_detections - n_match\n",
    "\n",
    "# POD, FAR, H, M, FA\n",
    "print(f\"\\nHits : {H}\")\n",
    "print(f\"Misses : {M}\")\n",
    "print(f\"False Alarms : {FA}\")\n",
    "print(f\"POD : {POD}\")\n",
    "print(f\"FAR : {FAR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Results\n",
    "\n",
    "In the paper, with the ML ensemble we have the following results:\n",
    "\n",
    "- F2-score : 53 %\n",
    "- Euclidean distance : 117.06 km\n",
    "- Hit rate : 88.91 %\n",
    "- POD : 71.49 %\n",
    "- FAR : 23.00 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
