{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931fc3f2-c2d5-4efe-82f6-e5ae19f7af0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import joblib\n",
    "import logging\n",
    "import munch\n",
    "import os\n",
    "import sys\n",
    "import toml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../resources/library')\n",
    "import tropical_cyclone as tc\n",
    "from tropical_cyclone.dataset import TCGraphDatasetInference\n",
    "from tropical_cyclone.macros import TEST_YEARS as test_years\n",
    "from tropical_cyclone.tester import GraphTester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a0f02-5db2-4e6d-935e-630576a82186",
   "metadata": {},
   "source": [
    "## Select experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b866d6-ffbf-45d5-a431-f726b0c27908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_dir = '../experiments/graphunet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9919d-9469-4c0a-9f9d-d19f5e02caca",
   "metadata": {},
   "source": [
    "## Configuration file parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a341e3-b92f-4d4c-bc67-e1dcfc725d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the dataset folder\n",
    "dataset_dir = '../data/dataset'\n",
    "\n",
    "# get configuration filename\n",
    "config_file = os.path.join(run_dir, 'configuration.toml')\n",
    "\n",
    "# parse config parameters\n",
    "config = munch.munchify(toml.load(config_file))\n",
    "\n",
    "# setup scaler\n",
    "scaler = joblib.load(config.dir.scaler)\n",
    "\n",
    "# data\n",
    "drivers = config.data.drivers\n",
    "targets = config.data.targets\n",
    "\n",
    "# train parameters\n",
    "batch_size = config.train.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd0eba5-0217-495d-9d39-c60524d31089",
   "metadata": {},
   "source": [
    "## Select model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1446bb6-5af8-4a02-93c2-b0b8bf687d71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch-0000-val_loss-0.12.ckpt\n",
      "1 epoch-0000-val_loss-0.13.ckpt\n",
      "2 epoch-0000-val_loss-0.31.ckpt\n",
      "3 epoch-0000-val_loss-0.36.ckpt\n",
      "4 last-v1.ckpt\n",
      "5 last-v2.ckpt\n",
      "6 last-v3.ckpt\n",
      "7 last.ckpt\n"
     ]
    }
   ],
   "source": [
    "# list all available checkpoints\n",
    "models = sorted(glob.glob(os.path.join(run_dir, 'checkpoints', '*.ckpt')))\n",
    "for idx,model_name in enumerate(models):\n",
    "    print(idx, model_name.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3146d1bf-ef37-4cf7-a546-98ff32bab750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../experiments/graphunet/checkpoints/last.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick your model\n",
    "model_file = models[-1]\n",
    "model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d66104-d57c-4f4c-89e9-b0c539ecafa7",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed55bf71-3520-4e50-a1fa-6bd6d012e2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model details\n",
    "model_cls = eval(config.model.cls)\n",
    "model_args = dict(config.model.args)\n",
    "\n",
    "# define device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# define model\n",
    "model:nn.Module = model_cls(**model_args)\n",
    "model = model.to(device)\n",
    "\n",
    "# load state dictionary\n",
    "state_dict = torch.load(f=model_file, map_location=device)\n",
    "\n",
    "# load weights into the model\n",
    "model.load_state_dict(state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8868e11-3dba-4996-b836-7d4d4b2dd523",
   "metadata": {},
   "source": [
    "## Directory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e549652f-7887-48b5-bc23-8c16ad038e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define inference folder\n",
    "inference_dir = os.path.join(run_dir, 'inference')\n",
    "\n",
    "# define logs directory\n",
    "log_dir = os.path.join(run_dir, 'logs_inference')\n",
    "\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8210c1-c497-470f-9c7a-1437403067b9",
   "metadata": {},
   "source": [
    "## Inference on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9059bc72-5812-4c0b-9543-76b0ea4b5444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2014 created with 131208 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference on the test set: 100%|██████████| 257/257 [01:49<00:00,  2.35batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2015 created with 148764 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 291/291 [02:06<00:00,  2.30batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2016 created with 112266 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 220/220 [01:20<00:00,  2.73batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2017 created with 120428 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 236/236 [01:31<00:00,  2.58batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2018 created with 135212 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 265/265 [01:45<00:00,  2.52batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2019 created with 133210 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 261/261 [01:42<00:00,  2.54batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2020 created with 113190 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 222/222 [01:23<00:00,  2.67batch/s]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tInference dataset for year 2021 created with 103796 elements!\n",
      "\tshape of elements:\n",
      "\t\tx: torch.Size([1600, 6])\n",
      "\t\tedge_index: torch.Size([2, 6240])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Inference on the test set: 100%|██████████| 203/203 [01:16<00:00,  2.64batch/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize logger\n",
    "logging_level = logging.INFO\n",
    "logging.basicConfig(format=\"[%(asctime)s] %(levelname)s : %(message)s\", filename=f\"{log_dir}/proc-{0}.log\", \n",
    "                    filemode=\"w\", level=logging_level, datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logging.info(f'Starting inference')\n",
    "\n",
    "for year in test_years:\n",
    "    logging.info(f'Year {year}')\n",
    "    \n",
    "    # creating graph dataset and dataloader for the current year\n",
    "    logging.info(f'  Dataset preparation...')\n",
    "    dataset = tc.dataset.TCGraphDatasetInference(src=dataset_dir, year=year, drivers=drivers, targets=targets, scaler=scaler)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # getting a graph tester to obtain the y predictions with shape [B, 2]\n",
    "    logging.info(f'  ...predicting...')\n",
    "    tester = GraphTester(device=device, loader=loader, model=model, nodes_per_graph=model_args['nodes_per_graph'])\n",
    "    tot_pred = tester.get_inference_y(threshold=0.4)\n",
    "    \n",
    "    # post-process operations\n",
    "    logging.info(f'  ...post-processing...')\n",
    "    dataset.post_process(tot_pred)\n",
    "    \n",
    "    # save .csv with coordinates and times to disk\n",
    "    detection_dst = os.path.join(inference_dir, f'{year}.csv')\n",
    "    dataset.store_detections(dst=detection_dst)\n",
    "    logging.info(f'  ...predictions stored!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
