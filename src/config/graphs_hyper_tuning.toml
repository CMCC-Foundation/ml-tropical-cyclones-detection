[run]
seed = 12368

[dir]
experiment = '../experiments'
run = '../experiments/graphunet_hyperparam_tuning_h3'
scaler = '../data/scalers/standard.dump'
train = '../data/patches/train'
valid = '../data/patches/valid'
test = '../data/patches/test'
result = 'result.pkl'

# add the result file name here


[torch]
matmul_precision = "medium"
dtype = 'torch.float32'

[lightning]
accelerator = "cuda" # "cpu"
precision = "32-true"

[data]
drivers = ['fg10', 'i10fg', 'msl', 't_500', 't_300', 'vo_850']
targets = ['density_map_tc']

[model]
cls = 'tc.models.GraphUNet'
metrics = []
    [model.args]
    in_channels = 6
    hid_channels = 64
    out_channels = 1
    K_pool = [800, 1600]
    nodes_per_graph = 1600 # 40x40 grids translate to 1600 nodes per graph
    edge_dropout_rate = 0.2
    node_dropout_rate = 0.6
    activation = 'nn.Sigmoid()'

[loss]
cls = 'nn.BCELoss'
    [loss.args]

[optimizer]

cls = ['torch.optim.Adam', 'torch.optim.Adagrad']#, 'torch.optim.SGD', 'torch.optim.RMSprop', 'torch.optim.Adagrad', 'torch.optim.Adadelta', 'torch.optim.Nadam', 'torch.optim.Adamax']

#cls = 'torch.optim.Adam'#, 'torch.optim.SGD', 'torch.optim.RMSprop', 'torch.optim.Adagrad', 'torch.optim.Adadelta', 'torch.optim.Nadam', 'torch.optim.Adamax']

 #   [optimizer.args]
  #  lr = 0.01

[scheduler]
    [scheduler.args]

[train]
epochs = 1
batch_size = 512
augmentation = true
drop_remainder = true
accumulation_steps = 1
